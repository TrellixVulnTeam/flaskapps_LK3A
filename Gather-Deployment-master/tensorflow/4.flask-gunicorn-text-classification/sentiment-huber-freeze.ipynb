{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 50\n",
    "location = os.getcwd()\n",
    "num_layers = 3\n",
    "size_layer = 256\n",
    "learning_rate = 0.0001\n",
    "batch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset-sentiment.p', 'rb') as fopen:\n",
    "    df = pickle.load(fopen)\n",
    "with open('vector-sentiment.p', 'rb') as fopen:\n",
    "    vectors = pickle.load(fopen)\n",
    "with open('dictionary-sentiment.p', 'rb') as fopen:\n",
    "    dictionary = pickle.load(fopen)\n",
    "label = np.unique(df[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(df[:,0], df[:, 1].astype('int'), test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, num_layers, size_layer, dimension_input, dimension_output, learning_rate):\n",
    "        def lstm_cell():\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layer)\n",
    "        self.rnn_cells = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)])\n",
    "        self.X = tf.placeholder(tf.float32, [None, None, dimension_input])\n",
    "        self.Y = tf.placeholder(tf.float32, [None, dimension_output])\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(self.rnn_cells, output_keep_prob = 0.5)\n",
    "        self.outputs, self.last_state = tf.nn.dynamic_rnn(drop, self.X, dtype = tf.float32)\n",
    "        self.rnn_W = tf.Variable(tf.random_normal((size_layer, dimension_output)))\n",
    "        self.rnn_B = tf.Variable(tf.random_normal([dimension_output]))\n",
    "        # put 'logits' name is very important\n",
    "        self.logits = tf.add(tf.matmul(self.outputs[:, -1], self.rnn_W),self.rnn_B,name='logits')\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.Y))\n",
    "        l2 = sum(0.0005 * tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "        self.cost += l2\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
    "        self.correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , pass acc: 0 , current acc: 0.629399988651\n",
      "time taken: 26.739943265914917\n",
      "epoch: 0 , training loss: 1.17874772638 , training acc: 0.57489997685 , valid loss: 1.05271718264 , valid acc: 0.629399988651\n",
      "epoch: 1 , pass acc: 0.629399988651 , current acc: 0.65119998455\n",
      "time taken: 26.51980972290039\n",
      "epoch: 1 , training loss: 1.0023892054 , training acc: 0.635899980366 , valid loss: 0.945737411976 , valid acc: 0.65119998455\n",
      "time taken: 26.43423104286194\n",
      "epoch: 2 , training loss: 0.926609389484 , training acc: 0.648699977696 , valid loss: 0.903479164839 , valid acc: 0.644799985886\n",
      "time taken: 26.50618577003479\n",
      "epoch: 3 , training loss: 0.886136015952 , training acc: 0.653449975252 , valid loss: 0.874164613485 , valid acc: 0.649799983501\n",
      "epoch: 4 , pass acc: 0.65119998455 , current acc: 0.663399980068\n",
      "time taken: 26.59169864654541\n",
      "epoch: 4 , training loss: 0.863515041471 , training acc: 0.66004997611 , valid loss: 0.850822336674 , valid acc: 0.663399980068\n",
      "time taken: 26.520148992538452\n",
      "epoch: 5 , training loss: 0.846785361767 , training acc: 0.662699973285 , valid loss: 0.844718412161 , valid acc: 0.661399980783\n",
      "epoch: 6 , pass acc: 0.663399980068 , current acc: 0.664199984074\n",
      "time taken: 26.624850511550903\n",
      "epoch: 6 , training loss: 0.83727365911 , training acc: 0.663749976456 , valid loss: 0.833394775391 , valid acc: 0.664199984074\n",
      "time taken: 26.518184423446655\n",
      "epoch: 7 , training loss: 0.830313061774 , training acc: 0.665149978697 , valid loss: 0.827816601992 , valid acc: 0.660599973202\n",
      "time taken: 26.502766370773315\n",
      "epoch: 8 , training loss: 0.823405617476 , training acc: 0.668049977422 , valid loss: 0.821383508444 , valid acc: 0.660999978781\n",
      "epoch: 9 , pass acc: 0.664199984074 , current acc: 0.664599978924\n",
      "time taken: 26.596304655075073\n",
      "epoch: 9 , training loss: 0.81641638726 , training acc: 0.666749974787 , valid loss: 0.818988826275 , valid acc: 0.664599978924\n",
      "epoch: 10 , pass acc: 0.664599978924 , current acc: 0.665199972391\n",
      "time taken: 26.61149287223816\n",
      "epoch: 10 , training loss: 0.810500105321 , training acc: 0.668199977279 , valid loss: 0.811666517258 , valid acc: 0.665199972391\n",
      "time taken: 26.514841318130493\n",
      "epoch: 11 , training loss: 0.804236527383 , training acc: 0.668749975264 , valid loss: 0.806621295214 , valid acc: 0.664599984884\n",
      "time taken: 26.523298740386963\n",
      "epoch: 12 , training loss: 0.799508748353 , training acc: 0.672649976313 , valid loss: 0.803265106678 , valid acc: 0.662599980831\n",
      "time taken: 26.529006481170654\n",
      "epoch: 13 , training loss: 0.794081473351 , training acc: 0.672199980319 , valid loss: 0.798970257044 , valid acc: 0.66119998455\n",
      "time taken: 26.532240629196167\n",
      "epoch: 14 , training loss: 0.788695190251 , training acc: 0.670399976075 , valid loss: 0.792740504742 , valid acc: 0.660199978352\n",
      "epoch: 15 , pass acc: 0.665199972391 , current acc: 0.667999979258\n",
      "time taken: 26.61776614189148\n",
      "epoch: 15 , training loss: 0.782855423093 , training acc: 0.671499975622 , valid loss: 0.786209183931 , valid acc: 0.667999979258\n",
      "time taken: 26.549970626831055\n",
      "epoch: 16 , training loss: 0.776873866022 , training acc: 0.669299974144 , valid loss: 0.780972168446 , valid acc: 0.666199973822\n",
      "time taken: 26.553519248962402\n",
      "epoch: 17 , training loss: 0.772861882448 , training acc: 0.673449973464 , valid loss: 0.778998032808 , valid acc: 0.666599980593\n",
      "time taken: 26.559773921966553\n",
      "epoch: 18 , training loss: 0.768877471387 , training acc: 0.670899977982 , valid loss: 0.772851135731 , valid acc: 0.666999977827\n",
      "time taken: 26.563251495361328\n",
      "epoch: 19 , training loss: 0.763463329077 , training acc: 0.675149977207 , valid loss: 0.771162133217 , valid acc: 0.664199981689\n",
      "time taken: 26.53854537010193\n",
      "epoch: 20 , training loss: 0.759607311189 , training acc: 0.674399977922 , valid loss: 0.772670954466 , valid acc: 0.653199976683\n",
      "time taken: 26.54240393638611\n",
      "epoch: 21 , training loss: 0.754829078019 , training acc: 0.673399973512 , valid loss: 0.764480924606 , valid acc: 0.663199975491\n",
      "time taken: 26.54299783706665\n",
      "epoch: 22 , training loss: 0.751469352841 , training acc: 0.676549977958 , valid loss: 0.760457355976 , valid acc: 0.66079998374\n",
      "time taken: 26.51711869239807\n",
      "epoch: 23 , training loss: 0.746324723363 , training acc: 0.676949977279 , valid loss: 0.757468924522 , valid acc: 0.658399976492\n",
      "time taken: 26.52518367767334\n",
      "epoch: 24 , training loss: 0.742790798843 , training acc: 0.677499979138 , valid loss: 0.754917628765 , valid acc: 0.659999979734\n",
      "time taken: 26.506306171417236\n",
      "epoch: 25 , training loss: 0.73800242126 , training acc: 0.677599976361 , valid loss: 0.752521883249 , valid acc: 0.663999985456\n",
      "break epoch: 26\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(num_layers, size_layer, vectors.shape[1], label.shape[0], learning_rate)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "dimension = vectors.shape[1]\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 10, 0, 0, 0\n",
    "while True:\n",
    "    lasttime = time.time()\n",
    "    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n",
    "        print('break epoch:', EPOCH)\n",
    "        break\n",
    "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
    "    for i in range(0, (train_X.shape[0] // batch) * batch, batch):\n",
    "        batch_x = np.zeros((batch, maxlen, dimension))\n",
    "        batch_y = np.zeros((batch, len(label)))\n",
    "        for k in range(batch):\n",
    "            tokens = train_X[i + k].split()[:maxlen]\n",
    "            emb_data = np.zeros((maxlen, dimension), dtype = np.float32)\n",
    "            for no, text in enumerate(tokens[::-1]):\n",
    "                try:\n",
    "                    emb_data[-1 - no, :] += vectors[dictionary[text], :]\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "            batch_y[k, int(train_Y[i + k])] = 1.0\n",
    "            batch_x[k, :, :] = emb_data[:, :]\n",
    "        loss, _ = sess.run([model.cost, model.optimizer], feed_dict = {model.X : batch_x, model.Y : batch_y})\n",
    "        train_loss += loss\n",
    "        train_acc += sess.run(model.accuracy, feed_dict = {model.X : batch_x, model.Y : batch_y})\n",
    "    \n",
    "    for i in range(0, (test_X.shape[0] // batch) * batch, batch):\n",
    "        batch_x = np.zeros((batch, maxlen, dimension))\n",
    "        batch_y = np.zeros((batch, len(label)))\n",
    "        for k in range(batch):\n",
    "            tokens = test_X[i + k].split()[:maxlen]\n",
    "            emb_data = np.zeros((maxlen, dimension), dtype = np.float32)\n",
    "            for no, text in enumerate(tokens[::-1]):\n",
    "                try:\n",
    "                    emb_data[-1 - no, :] += vectors[dictionary[text], :]\n",
    "                except:\n",
    "                    continue\n",
    "            batch_y[k, int(test_Y[i + k])] = 1.0\n",
    "            batch_x[k, :, :] = emb_data[:, :]\n",
    "        loss, acc = sess.run([model.cost, model.accuracy], feed_dict = {model.X : batch_x, model.Y : batch_y})\n",
    "        test_loss += loss\n",
    "        test_acc += acc\n",
    "        \n",
    "    train_loss /= (train_X.shape[0] // batch)\n",
    "    train_acc /= (train_X.shape[0] // batch)\n",
    "    test_loss /= (test_X.shape[0] // batch)\n",
    "    test_acc /= (test_X.shape[0] // batch)\n",
    "    if test_acc > CURRENT_ACC:\n",
    "        print('epoch:', EPOCH, ', pass acc:', CURRENT_ACC, ', current acc:', test_acc)\n",
    "        CURRENT_ACC = test_acc\n",
    "        CURRENT_CHECKPOINT = 0\n",
    "        saver.save(sess, os.getcwd() + \"/model-rnn-vector-huber.ckpt\")\n",
    "    else:\n",
    "        CURRENT_CHECKPOINT += 1\n",
    "    print('time taken:', time.time()-lasttime)\n",
    "    print('epoch:', EPOCH, ', training loss:', train_loss, ', training acc:', train_acc, ', valid loss:', test_loss, ', valid acc:', test_acc)\n",
    "    EPOCH += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only load Variables, placeholder for input, and our logits\n",
    "strings=','.join([n.name for n in tf.get_default_graph().as_graph_def().node if \"Variable\" in n.op or n.name.find('Placeholder') >= 0 or n.name.find('logits') == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            \"directory: %s\" % model_dir)\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "    \n",
    "    absolute_model_dir = \"/\".join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + \"/frozen_model.pb\"\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(\",\")\n",
    "        ) \n",
    "        with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print(\"%d ops in the final graph.\" % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/husein/space/text-dataset/sentiment/model-rnn-vector-huber.ckpt\n",
      "INFO:tensorflow:Froze 26 variables.\n",
      "Converted 26 variables to const ops.\n",
      "247 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph(\"\", strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=load_graph('frozen_model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import/Placeholder\n",
      "import/Placeholder_1\n",
      "import/Rank\n",
      "import/range/start\n",
      "import/range/delta\n",
      "import/range\n",
      "import/concat/values_0\n",
      "import/concat/axis\n",
      "import/concat\n",
      "import/transpose\n",
      "import/rnn/Shape\n",
      "import/rnn/strided_slice/stack\n",
      "import/rnn/strided_slice/stack_1\n",
      "import/rnn/strided_slice/stack_2\n",
      "import/rnn/strided_slice\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState/ExpandDims/dim\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState/ExpandDims\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState/Const\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState/concat/axis\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState/concat\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState/zeros/Const\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState/zeros\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState/ExpandDims_2/dim\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState/ExpandDims_2\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState/Const_2\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState/concat_1/axis\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState/concat_1\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState/zeros_1/Const\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState/zeros_1\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_1/ExpandDims/dim\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_1/ExpandDims\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_1/Const\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_1/concat/axis\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_1/concat\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_1/zeros/Const\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_1/zeros\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_1/ExpandDims_2/dim\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_1/ExpandDims_2\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_1/Const_2\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_1/concat_1/axis\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_1/concat_1\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_1/zeros_1/Const\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_1/zeros_1\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_2/ExpandDims/dim\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_2/ExpandDims\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_2/Const\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_2/concat/axis\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_2/concat\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_2/zeros/Const\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_2/zeros\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_2/ExpandDims_2/dim\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_2/ExpandDims_2\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_2/Const_2\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_2/concat_1/axis\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_2/concat_1\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_2/zeros_1/Const\n",
      "import/rnn/DropoutWrapperZeroState/MultiRNNCellZeroState/LSTMCellZeroState_2/zeros_1\n",
      "import/rnn/Shape_1\n",
      "import/rnn/strided_slice_2/stack\n",
      "import/rnn/strided_slice_2/stack_1\n",
      "import/rnn/strided_slice_2/stack_2\n",
      "import/rnn/strided_slice_2\n",
      "import/rnn/time\n",
      "import/rnn/TensorArray\n",
      "import/rnn/TensorArray_1\n",
      "import/rnn/TensorArrayUnstack/Shape\n",
      "import/rnn/TensorArrayUnstack/strided_slice/stack\n",
      "import/rnn/TensorArrayUnstack/strided_slice/stack_1\n",
      "import/rnn/TensorArrayUnstack/strided_slice/stack_2\n",
      "import/rnn/TensorArrayUnstack/strided_slice\n",
      "import/rnn/TensorArrayUnstack/range/start\n",
      "import/rnn/TensorArrayUnstack/range/delta\n",
      "import/rnn/TensorArrayUnstack/range\n",
      "import/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3\n",
      "import/rnn/while/Enter\n",
      "import/rnn/while/Enter_1\n",
      "import/rnn/while/Enter_2\n",
      "import/rnn/while/Enter_3\n",
      "import/rnn/while/Enter_4\n",
      "import/rnn/while/Enter_5\n",
      "import/rnn/while/Enter_6\n",
      "import/rnn/while/Enter_7\n",
      "import/rnn/while/Merge\n",
      "import/rnn/while/Merge_1\n",
      "import/rnn/while/Merge_2\n",
      "import/rnn/while/Merge_3\n",
      "import/rnn/while/Merge_4\n",
      "import/rnn/while/Merge_5\n",
      "import/rnn/while/Merge_6\n",
      "import/rnn/while/Merge_7\n",
      "import/rnn/while/Less/Enter\n",
      "import/rnn/while/Less\n",
      "import/rnn/while/LoopCond\n",
      "import/rnn/while/Switch\n",
      "import/rnn/while/Switch_1\n",
      "import/rnn/while/Switch_2\n",
      "import/rnn/while/Switch_3\n",
      "import/rnn/while/Switch_4\n",
      "import/rnn/while/Switch_5\n",
      "import/rnn/while/Switch_6\n",
      "import/rnn/while/Switch_7\n",
      "import/rnn/while/Identity\n",
      "import/rnn/while/Identity_1\n",
      "import/rnn/while/Identity_2\n",
      "import/rnn/while/Identity_3\n",
      "import/rnn/while/Identity_4\n",
      "import/rnn/while/Identity_5\n",
      "import/rnn/while/Identity_6\n",
      "import/rnn/while/Identity_7\n",
      "import/rnn/while/TensorArrayReadV3/Enter\n",
      "import/rnn/while/TensorArrayReadV3/Enter_1\n",
      "import/rnn/while/TensorArrayReadV3\n",
      "import/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel\n",
      "import/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/read\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/lstm_cell/concat/axis\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/lstm_cell/concat\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/lstm_cell/MatMul/Enter\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/lstm_cell/MatMul\n",
      "import/rnn/multi_rnn_cell/cell_0/lstm_cell/bias\n",
      "import/rnn/multi_rnn_cell/cell_0/lstm_cell/bias/read\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/lstm_cell/BiasAdd/Enter\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/lstm_cell/BiasAdd\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/split/split_dim\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/split\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/add/y\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/add\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/Sigmoid\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/mul\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/Sigmoid_1\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/Tanh\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/mul_1\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/add_1\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/Sigmoid_2\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/Tanh_1\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/lstm_cell/mul_2\n",
      "import/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel\n",
      "import/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/read\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell/lstm_cell/concat/axis\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell/lstm_cell/concat\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell/lstm_cell/MatMul/Enter\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell/lstm_cell/MatMul\n",
      "import/rnn/multi_rnn_cell/cell_1/lstm_cell/bias\n",
      "import/rnn/multi_rnn_cell/cell_1/lstm_cell/bias/read\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell/lstm_cell/BiasAdd/Enter\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell/lstm_cell/BiasAdd\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell/split/split_dim\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell/split\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell/add/y\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell/add\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell/Sigmoid\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell/mul\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell/Sigmoid_1\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell/Tanh\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell/mul_1\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell/add_1\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell/Sigmoid_2\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell/Tanh_1\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/lstm_cell/lstm_cell/mul_2\n",
      "import/rnn/multi_rnn_cell/cell_2/lstm_cell/kernel\n",
      "import/rnn/multi_rnn_cell/cell_2/lstm_cell/kernel/read\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_2/cell_2/lstm_cell/lstm_cell/lstm_cell/concat/axis\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_2/cell_2/lstm_cell/lstm_cell/lstm_cell/concat\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_2/cell_2/lstm_cell/lstm_cell/lstm_cell/MatMul/Enter\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_2/cell_2/lstm_cell/lstm_cell/lstm_cell/MatMul\n",
      "import/rnn/multi_rnn_cell/cell_2/lstm_cell/bias\n",
      "import/rnn/multi_rnn_cell/cell_2/lstm_cell/bias/read\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_2/cell_2/lstm_cell/lstm_cell/lstm_cell/BiasAdd/Enter\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_2/cell_2/lstm_cell/lstm_cell/lstm_cell/BiasAdd\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_2/cell_2/lstm_cell/lstm_cell/split/split_dim\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_2/cell_2/lstm_cell/lstm_cell/split\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_2/cell_2/lstm_cell/lstm_cell/add/y\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_2/cell_2/lstm_cell/lstm_cell/add\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_2/cell_2/lstm_cell/lstm_cell/Sigmoid\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_2/cell_2/lstm_cell/lstm_cell/mul\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_2/cell_2/lstm_cell/lstm_cell/Sigmoid_1\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_2/cell_2/lstm_cell/lstm_cell/Tanh\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_2/cell_2/lstm_cell/lstm_cell/mul_1\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_2/cell_2/lstm_cell/lstm_cell/add_1\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_2/cell_2/lstm_cell/lstm_cell/Sigmoid_2\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_2/cell_2/lstm_cell/lstm_cell/Tanh_1\n",
      "import/rnn/while/rnn/multi_rnn_cell/cell_2/cell_2/lstm_cell/lstm_cell/mul_2\n",
      "import/rnn/while/dropout/keep_prob\n",
      "import/rnn/while/dropout/Shape\n",
      "import/rnn/while/dropout/random_uniform/min\n",
      "import/rnn/while/dropout/random_uniform/max\n",
      "import/rnn/while/dropout/random_uniform/RandomUniform\n",
      "import/rnn/while/dropout/random_uniform/sub\n",
      "import/rnn/while/dropout/random_uniform/mul\n",
      "import/rnn/while/dropout/random_uniform\n",
      "import/rnn/while/dropout/add\n",
      "import/rnn/while/dropout/Floor\n",
      "import/rnn/while/dropout/div\n",
      "import/rnn/while/dropout/mul\n",
      "import/rnn/while/TensorArrayWrite/TensorArrayWriteV3/Enter\n",
      "import/rnn/while/TensorArrayWrite/TensorArrayWriteV3\n",
      "import/rnn/while/add/y\n",
      "import/rnn/while/add\n",
      "import/rnn/while/NextIteration\n",
      "import/rnn/while/NextIteration_1\n",
      "import/rnn/while/NextIteration_2\n",
      "import/rnn/while/NextIteration_3\n",
      "import/rnn/while/NextIteration_4\n",
      "import/rnn/while/NextIteration_5\n",
      "import/rnn/while/NextIteration_6\n",
      "import/rnn/while/NextIteration_7\n",
      "import/rnn/while/Exit_1\n",
      "import/rnn/TensorArrayStack/TensorArraySizeV3\n",
      "import/rnn/TensorArrayStack/range/start\n",
      "import/rnn/TensorArrayStack/range/delta\n",
      "import/rnn/TensorArrayStack/range\n",
      "import/rnn/TensorArrayStack/TensorArrayGatherV3\n",
      "import/rnn/Rank\n",
      "import/rnn/range/start\n",
      "import/rnn/range/delta\n",
      "import/rnn/range\n",
      "import/rnn/concat_1/values_0\n",
      "import/rnn/concat_1/axis\n",
      "import/rnn/concat_1\n",
      "import/rnn/transpose\n",
      "import/Variable\n",
      "import/Variable/read\n",
      "import/Variable_1\n",
      "import/Variable_1/read\n",
      "import/strided_slice/stack\n",
      "import/strided_slice/stack_1\n",
      "import/strided_slice/stack_2\n",
      "import/strided_slice\n",
      "import/MatMul\n",
      "import/logits\n",
      "import/beta1_power\n",
      "import/beta2_power\n",
      "import/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam\n",
      "import/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1\n",
      "import/rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam\n",
      "import/rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1\n",
      "import/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam\n",
      "import/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam_1\n",
      "import/rnn/multi_rnn_cell/cell_1/lstm_cell/bias/Adam\n",
      "import/rnn/multi_rnn_cell/cell_1/lstm_cell/bias/Adam_1\n",
      "import/rnn/multi_rnn_cell/cell_2/lstm_cell/kernel/Adam\n",
      "import/rnn/multi_rnn_cell/cell_2/lstm_cell/kernel/Adam_1\n",
      "import/rnn/multi_rnn_cell/cell_2/lstm_cell/bias/Adam\n",
      "import/rnn/multi_rnn_cell/cell_2/lstm_cell/bias/Adam_1\n",
      "import/Variable/Adam\n",
      "import/Variable/Adam_1\n",
      "import/Variable_1/Adam\n",
      "import/Variable_1/Adam_1\n"
     ]
    }
   ],
   "source": [
    "for op in g.get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = g.get_tensor_by_name('import/Placeholder:0')\n",
    "y = g.get_tensor_by_name('import/logits:0')\n",
    "test_sess = tf.InteractiveSession(graph=g)\n",
    "results = np.argmax(test_sess.run(tf.nn.softmax(y), feed_dict={x:batch_x}),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
