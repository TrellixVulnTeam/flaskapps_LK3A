<!doctype html>
<html lang="en">
<head>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="description" content="Hey, Jetson! Automatic Speech Recogntion Inference with TensorFlow/Keras on the Nvidia Jetson">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <title>Hey, Jetson!</title>
    <meta name="author" content="Brice Walker">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:regular,bold,italic,thin,light,bolditalic,black,medium&amp;lang=en">
    <link rel="stylesheet" href="https://code.getmdl.io/1.3.0/material.deep_purple-light_blue.min.css" />
    <link rel= "stylesheet" href="{{ url_for('static', filename='styles/styles.css') }}">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    <link rel= "stylesheet" href="{{ url_for('static', filename='fa/css/fontawesome.min.css') }}">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.8/css/all.css" integrity="sha384-3AB7yXWz4OeoZcPbieVW64vVXEwADiYyAEhwilzWsLw+9FgqpyjjStpPnpBO8o8S" crossorigin="anonymous">
    <link rel="apple-touch-icon" sizes="180x180" href="{{ url_for('static', filename='icons/apple-touch-icon.png') }}">
    <link rel="icon" type="image/png" sizes="32x32" href="{{ url_for('static', filename='icons/favicon-32x32.png') }}">
    <link rel="icon" type="image/png" sizes="16x16" href="{{ url_for('static', filename='icons/favicon-16x16.png') }}">
    <link rel="manifest" href="{{ url_for('static', filename='icons/site.webmanifest') }}">
    <link rel="mask-icon" href="{{ url_for('static', filename='icons/safari-pinned-tab.svg') }}" color="#5bbad5">
    <link rel="shortcut icon" href="{{ url_for('static', filename='icons/favicon.ico') }}">
    <meta name="apple-mobile-web-app-title" content="Hey, Jetson!">
    <meta name="application-name" content="Hey, Jetson!">
    <meta name="msapplication-TileColor" content="#00a300">
    <meta name="msapplication-config" content="{{ url_for('static', filename='icons/browserconfig.xml') }}">
    <meta name="theme-color" content="#ffffff">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-90685436-2"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-90685436-2');
    </script>
</head>

<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header">
        <header class="mdl-layout__header mdl-layout__header--waterfall portfolio-header">
            <div class="mdl-layout__header-row portfolio-logo-row">
                <span class="mdl-layout__title">
                    <span class="mdl-layout__title">Hey, Jetson!</span>
                </span>
            </div>
            <div class="mdl-layout__header-row portfolio-navigation-row mdl-layout--large-screen-only">
                <nav class="mdl-navigation mdl-typography--body-1-force-preferred-font">
                    <a class="mdl-navigation__link" href="index.html">Inference</a>
                    <a class="mdl-navigation__link" href="asr.html">ASR</a>
                    <a class="mdl-navigation__link" href="visualization.html">Visualization</a>
                    <a class="mdl-navigation__link" href="performance.html">Performance</a>
                    <a class="mdl-navigation__link" href="sentiment.html">Sentiment</a>
                    <a class="mdl-navigation__link is-active" href="about.html">About</a>
                    <a class="mdl-navigation__link" href="contact.html">Contact</a>
                </nav>
            </div>
            </header>
            <div class="mdl-layout__drawer mdl-layout--small-screen-only">
                <nav class="mdl-navigation mdl-typography--body-1-force-preferred-font">
                    <a class="mdl-navigation__link" href="index.html">Inference</a>
                    <a class="mdl-navigation__link" href="asr.html">ASR</a>
                    <a class="mdl-navigation__link" href="visualization.html">Visualization</a>
                    <a class="mdl-navigation__link" href="performance.html">Performance</a>
                    <a class="mdl-navigation__link" href="sentiment.html">Sentiment</a>
                    <a class="mdl-navigation__link is-active" href="about.html">About</a>
                    <a class="mdl-navigation__link" href="contact.html">Contact</a>
                </nav>
            </div>
            <main class="mdl-layout__content">
                <div class="mdl-grid portfolio-max-width">
                    <div class="mdl-cell mdl-cell--12-col mdl-card mdl-shadow--4dp">
                        <div class="mdl-card__title">
                            <h2 class="mdl-card__title-text">Automatic Speech Recognition Inference</h2>
                        </div>
                        <div class="mdl-card__media">
                            <img class="article-image" src="{{ url_for('static', filename='images/raw.png') }}" border="0" alt="">
                        </div>
                        <div class="mdl-card__supporting-text">
                            <strong>Includes:</strong>
                                <span>Ubuntu, Python, Anaconda, HTML, CSS, JavaScript, VS Code, Jupyter Notebook, Keras, TensorFlow, Sci-kit Learn, matplotlib, seaborn, Azure Cognitive Services, gunicorn, CUDA, cuDNN, NGINX, Supervisor, & Certbot</span>
                        </div>
                        <div class="mdl-grid portfolio-copy">
                            <h3 class="mdl-cell mdl-cell--12-col mdl-typography--headline">About</h3>
                            <div class="mdl-cell mdl-cell--6-col">
                                <p>
                                    The inference engine on this website allows you to test real time speech recognition inference using a deep neural network on an Nvidia Jetson. The model consists of 3 layers of 256 convolutional neurons, 7 layers of 512 bidirectional recurrent neurons, an attention mechanism, and 2 layers of time distributed dense neurons. The model was trained using Keras/Tensorflow on an Nvidia GTX 1070 GPU and deployed on an Nvidia Jetson based web server using flask in python.
                                </p>
                            </div>
                            <div class="mdl-cell mdl-cell--6-col">
                                <img class="article-image" src="{{ url_for('static', filename='images/JTX2.png') }}" border="0" alt="">
                            </div>
                            <div class="mdl-cell mdl-cell--6-col">
                                <a href="https://github.com/bricewalker/Hey-Jetson">
                                    <button class="mdl-button mdl-js-button mdl-button--raised mdl-js-ripple-effect mdl-button--accent">
                                        Github
                                    </button>
                                </a>
                            </div>
                            <div class="mdl-cell mdl-cell--6-col">
                                <a href="https://nbviewer.jupyter.org/github/bricewalker/Hey-Jetson/blob/master/Speech.ipynb">
                                    <button class="mdl-button mdl-js-button mdl-button--raised mdl-js-ripple-effect mdl-button--accent">
                                        nbviewer
                                    </button>
                                </a>
                            </div>
                            <h3 class="mdl-cell mdl-cell--12-col mdl-typography--headline">Problem Scope</h3>
                            <div class="mdl-cell mdl-cell--12-col">
                                <p>
                                    The initial idea for this project was to explore the possibility of building a deep learning based speech recognition platform that could identify keywords in conversational speech, link them to therapeutic interventions and then conduct sentiment or emotion analysis to give real time feedback on client responses to cognitive behavioral talk therapy interventions.
                                </p>
                            </div>
                            <h3 class="mdl-cell mdl-cell--12-col mdl-typography--headline">Problem Statment</h3>
                            <div class="mdl-cell mdl-cell--12-col">
                                <p>
                                    My goal was to build a character-level ASR system using a recurrent neural network with attention in TensorFlow that can run inference on an Nvidia Jetson with a word error rate of < 20%. 
                                </p>
                            </div>
                            <h3 class="mdl-cell mdl-cell--12-col mdl-typography--headline">The Data</h3>
                            <div class="mdl-cell mdl-cell--12-col">
                                <p>
                                    The primary dataset used is the <a href="http://www.openslr.org/12/">LibriSpeech ASR corpus</a> which includes 1000 hours of recorded speech. The final model was trained on a 960 hour subset. The dataset consists of 16kHz audio files between 2-15 seconds long of spoken English derived from read audiobooks from the LibriVox project. The audio files were converted to single channel (mono) WAV/WAVE files (.wav extension) with a 64k bit rate, and a 16kHz sample rate. They were encoded in PCM format, and then cut/padded to an equal length of 10 seconds. The pre-processing techniques used for the text transcriptions included the removal of any punctuation other than apostrophes, and transforming all characters to lowercase. An overview of some of the difficulties of working with data such as this can be found <a href="https://awni.github.io/speech-recognition/">here</a>.
                                </p>
                            </div>
                            <h3 class="mdl-cell mdl-cell--12-col mdl-typography--headline">The Tools</h3>
                            <div class="mdl-cell mdl-cell--12-col">
                                <div class="mdl-cell mdl-cell--6-col">
                                    <img class="article-image" src="{{ url_for('static', filename='images/1070.jpg') }}" border="0" alt="">
                                </div>
                                <p>
                                    The training server contains an Intel 7700k, overclocked to 4.8GHz with 32Gb ram clocked at 2400Hz, with an <a href="https://www.nvidia.com/en-us/geforce/products/10series/geforce-gtx-1070-ti/">Nvidia GTX1070</a> clocked to 1746Mhz (1920 Pascal Cores). The inference server is a <a href="https://developer.nvidia.com/embedded/buy/jetson-tx2">Jetson TX2 Developer Kit</a> (256 Pascal Cores). The Jetson board is relatively inexpensive, while incorporating an internal GPU capable of running deep neural networks. This would allow running speech recognition on device in situations where privacy and security concerns make cloud computing less than ideal.
                                </p>
                            </div>
                            <h3 class="mdl-cell mdl-cell--12-col mdl-typography--headline">Feature Extraction and Engineering</h3>
                            <div class="mdl-cell mdl-cell--12-col">
                                <p>
                                    There are 3 primary methods for extracting features for speech recognition. This includes using raw audio forms, spectrograms, and mfcc's. For this project, I have created a character level sequence-to-sequence model using spectrograms. This allows me to train a model on a data set with a limited vocabulary that can generalize to more unique/rare words better. This comes at the cost of making a model that is more; computationally expensive, difficult to interpret/understand, and susceptible to the problems of vanishing or exploding gradients as the sequences can be quite long.
                                </p>
                                <h4>Raw Audio Waves (pictured above)</h4>
                                <p>This method uses the raw wave forms of the audio files and is a 1D vector where X = [x1, x2, x3...]</p>
                                <h4>Spectrograms</h4>
                                    <div class="mdl-cell mdl-cell--12-col">
                                        <img class="article-image" src="{{ url_for('static', filename='images/spectrogram.png') }}" border="0" alt="spectrogram">
                                    </div>
                                <p>
                                    This transforms the raw audio wave forms into a 2D tensor (using the Fourier transform) where the first dimension corresponds to time (the horizontal axis), and the second dimension corresponds to frequency (the vertical axis) rather than amplitude. We lose a little bit of information in this conversion process as we take the log of the power of FFT. This can be written as log |FFT(X)|^2. This gives us 161 features, so each feature corresponds to something between 99-100 Hz. The full transformation process is documented <a href="{{ url_for('static', filename='images/spectrograms.pdf') }}">here</a>.
                                </p>
                                <div class="mdl-cell mdl-cell--12-col">
                                    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
                                        {{ spectrogram_3d  }}
                                </div>
                                <h4>Mel-Frequency Cepstrum Coefficients</h4>
                                <div class="mdl-cell mdl-cell--12-col">
                                    <img class="article-image" src="{{ url_for('static', filename='images/mfcc.png') }}" border="0" alt="mfcc">
                                </div>
                                <p>
                                    Like the spectrogram, this turns the audio wave form into a 2D array. This works by mapping the powers of the Fourier transform of the signal, and then taking the discrete cosine transform of the logged mel powers. This produces a 2D array with reduced dimensions when compared to spectrograms, effectively allowing for compression of the spectrogram and speeding up training as we are left with 13 features. The full process for deriving MFCC's from audio is outlined <a href="{{ url_for('static', filename='images/mfccs.pdf') }}">here</a>.
                                </p>
                            </div>
                            <h3 class="mdl-cell mdl-cell--12-col mdl-typography--headline">Recurrent Neural Networks</h3>
                            <div class="mdl-cell mdl-cell--12-col">
                                <p>
                                    For this project, the architecture chosen is a (Recurrent) Deep Neural Network (RNN) as it is easy to implement, and scales well. At its core, this is a machine translation problem, so an encoder-decoder model is an appropriate framework choice. Recurrent neurons are similar to feedforward neurons, except they also have connections pointing backward. At each step in time, each neuron receives an input as well as its own output from the previous time step. Each neuron has two sets of weights, one for the input and one for the output at the last time step. Each layer takes vectors as inputs and outputs some vector. This model works by calculating forward propagation through each time step, t, and then back propagation through each time step. This is a 29 class classification problem. At each time step, the speaker is assumed to have spoken 1 of 29 possible characters (26 letters, 1 space character, 1 apostrophe, and 1 blank/empty character used to pad short files since inputs will have varying length). The output of this model at each time step will be a list of probabilities for each possible character.
                                </p>  
                                <p>
                                    Hey, Jetson! is comprised of a combined acoustic model and language model. The acoustic model scores sequences of acoustic model labels over a time frame and the language model scores sequences of characters. A decoding graph then maps valid acoustic label sequences to the corresponding character sequences. Speech recognition is a path search algorithm through the decoding graph, where the score of the path is the sum of the score given to it by the decoding graph, and the score given to it by the acoustic model. So, to put it simply, speech recognition is the process of finding the character sequence that maximizes both the language and acoustic model scores. For more information on the use of deep learning in speech recognition, read George Dahl's <a href="{{ url_for('static', filename='images/deeplearning.pdf') }}">paper</a>. You can also find more information on the use of recurrent neural networks in speech recognition in Alex Graves' <a href="{{ url_for('static', filename='images/rnn.pdf') }}">paper</a>.
                                </p>
                            </div>
                            <h3 class="mdl-cell mdl-cell--12-col mdl-typography--headline">Model Architecture</h3>
                            <div class="mdl-cell mdl-cell--12-col">
                                <p>
                                    <a href="{{ url_for('static', filename='images/model_11.png') }}">Model Graph.</a>
                                </p>
                                <p>
                                    With each epoch taking around 5 hours and 10 minutes to train using spectrograms, the total training time on an Nvidia GTX1070(8GB) for the final model architecture was roughly 6.5 days.
                                </p>
                                <p>
                                    A more detailed map of the model architecture can be found <a href="{{ url_for('static', filename='images/graph.png') }}">here</a>.
                                </p>
                            </div>
                            <h3 class="mdl-cell mdl-cell--12-col mdl-typography--headline">Convolutional Neurons</h3>
                            <div class="mdl-cell mdl-cell--12-col">
                                <p>
                                    The deep neural network in this project explores the use of a Convolutional Neural Network consisting of 256 neurons for early pattern detection. The initial layer of convolutional neurons conducts feature extraction for the recurrent network.
                                </p>
                            </div>
                            <h3 class="mdl-cell mdl-cell--12-col mdl-typography--headline">Dilated Convolutions</h3>
                            <div class="mdl-cell mdl-cell--12-col">
                                <p>
                                    The model also uses a dilated CNN layer. Dilation introduces gaps into the CNN's kernels, so that the receptive field must encircle areas rather than simply slide over the window in a systematic way. This means that the convolutional layer can pick up on the global context of what it is looking at while still only having as many weights/inputs as the standard form. Inspiration for this technique came from <a href="{{ url_for('static', filename='images/dilation.pdf') }}">IBM's Watson Team</a>.
                                </p>
                            </div>
                            <h3 class="mdl-cell mdl-cell--12-col mdl-typography--headline">Batch Normalization</h3>
                            <div class="mdl-cell mdl-cell--12-col">
                                <p>
                                    Hey, Jetson! also uses batch normalization, which normalizes the activations of the layers with a mean close to 0 and standard deviation close to 1. This reduces gradient expansion and prevents the network from overfitting.
                                </p>
                            </div>
                            <h3 class="mdl-cell mdl-cell--12-col mdl-typography--headline">LSTM/GRU Cells</h3>
                            <div class="mdl-cell mdl-cell--12-col">
                                <p>
                                    My RNN explores the use of layers of Long-Short Term Memory Cells and Gated Recurrent Units. LSTM's include forget and output gates, which allow more control over the cell's memory by allowing separate control of what is forgotten and what is passed through to the next hidden layer of cells. GRU's are a simplified type of Long-Short Term Memory Recurrent Neuron with fewer parameters than typical LSTM's. These work via a single memory update gate and provide most of the performance of traditional LSTM's at a fraction of the computing cost. For more information on LSTM's, read this <a href="{{ url_for('static', filename='images/lstm.pdf') }}">paper</a>, and for more information on GRU's, read this check out this <a href="{{ url_for('static', filename='images/gru.pdf') }}">paper</a>.
                                </p>
                            </div>
                            <h3 class="mdl-cell mdl-cell--12-col mdl-typography--headline">Bidirectional Layers</h3>
                            <div class="mdl-cell mdl-cell--12-col">
                                <p>
                                    This model explores connecting two hidden layers of opposite directions to the same output, making their future input information reachable from the current state. To put it simply, this creates two layers of neurons; 1 that goes through the sequence forward in time and 1 that goes through it backward through time. This allows the output layer to get information from past and future states meaning that it will have knowledge of the letters located before and after the current utterance. This can lead to great improvements in performance but comes at a cost of increased latency. Bidirectional layers are further outlined <a href="{{ url_for('static', filename='images/bidirectional.pdf') }}">here</a>.
                                </p>
                            </div>
                            <h3 class="mdl-cell mdl-cell--12-col mdl-typography--headline">Recurrent Dropout</h3>
                            <div class="mdl-cell mdl-cell--12-col">
                                <p>
                                    This model also employs randomized dropout of units to prevent the model from over fitting. An example of the use of dropout in ASR can be found in this paper <a href="{{ url_for('static', filename='images/microsoft2016.pdf') }}">paper</a>.
                                </p>
                            </div>
                            <h3 class="mdl-cell mdl-cell--12-col mdl-typography--headline">Attention Mechanism</h3>
                            <div class="mdl-cell mdl-cell--12-col">
                                <p>
                                    The decoder portion of the model includes the ability to "attend" to different parts of the audio clip at each time step. This lets the model learn what to pay attention to based on the input and what it has predicted the output to be so far. <a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/">Attention</a> allows the network to refer back to the input sequence by giving the network access to its internal memory, which is the hidden state of the encoder (the RNN layers).
                                </p>
                            </div>
                            <h3 class="mdl-cell mdl-cell--12-col mdl-typography--headline">Time Distributed Dense Layers</h3>
                            <div class="mdl-cell mdl-cell--12-col">
                                <p>
                                    The ASR model explores the addition of layers of normal Dense neurons where the neurons have been distributed across every temporal slice of the input sequence. For more info on TimeDistributed Layers, check out the <a href="https://keras.io/layers/wrappers/">Keras Documentation</a>.
                                </p>
                            </div>
                            <h3 class="mdl-cell mdl-cell--12-col mdl-typography--headline">Loss Function</h3>
                            <div class="mdl-cell mdl-cell--12-col">
                                <p>
                                    The loss function I am using is a custom implementation of Connectionist Temporal Classification (CTC), which is a special case of sequential objective functions that addresses some of the modeling burden in cross-entropy that forces the model to link every frame of input data to a label. CTC's label set includes a "blank" symbol in its alphabet so if a frame of data doesn’t contain any utterance, the CTC system can output "blank" indicating that there isn't enough information to classify an output. This also has the added benefits of allowing us to have inputs/outputs of varying length as short files can be padded with the "blank" character. This function only observes the sequence of labels along a path, ignoring the alignment of the labels to the acoustic data. More information on CTC can be found in Alex Grave's <a href="{{ url_for('static', filename='images/ctc.pdf') }}">paper</a>.
                                </p>
                            </div>
                            <h3 class="mdl-cell mdl-cell--12-col mdl-typography--headline">Performance</h3>
                            <div class="mdl-cell mdl-cell--12-col">
                                <img class="article-image" src="{{ url_for('static', filename='images/performance.png') }}" border="0" alt="">
                            </div>
                            <div class="mdl-cell mdl-cell--12-col">
                                <p>
                                    Language modeling, the component of a speech recognition system that estimates the prior probabilities of spoken sounds, is the system's knowledge of what probable character sequences are. This system uses a class based language model, which allows it to narrow down its search field through the vocabulary of the speech recognizer (the first part of the system) as it will rarely see a sentence that looks like "the dog the ate sand the water" so it will assume that 'the' is not likely to come after the word 'sand'. We do this by assigning a probability to every possible sentence and then picking the word or character with the highest prior probability of occurring. Language model smoothing (often called discounting) will help us overcome the problem that this creates a model that will assign a probability of 0 to anything it hasn't witnessed in training. This is done by distributing non-zero probabilities over all possible occurrences in proportion to the unigram probabilities of characters. This overcomes the limitations of traditional n-gram based modeling and is all made possible by the added dimension of time sequences in the recurrent neural network. More information on comparing models can be found in this <a href="{{ url_for('static', filename='images/comparingmodels.pdf') }}">paper</a>.
                                </p>
                                <p>
                                    The best performing model is considered the one that gives the highest probabilities to the characters that are found in a test set, since it wastes less probability on characters that don't actually occur. The overall <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a> of the model's predictions with the ground truth transcriptions in the validation set is around 80% and in the test set is about 78%, while the overall <a href="https://en.wikipedia.org/wiki/Word_error_rate">word error rate</a> in the validation set is around 16% and in the test set is about 18%. For inference, the model can produce text transcriptions within a range between 1-5 seconds, allowing for sub real-time inferencing.
                                </p>
                            </div>
                            <h3 class="mdl-cell mdl-cell--12-col mdl-typography--headline">Deployment</h3>
                            <div class="mdl-cell mdl-cell--12-col">
                                <p>
                                    The project is deployed to the web as a flask web app using a python based REST API. The web app is built using HTML and CSS and then published using a gunicorn server and NGINX reverse proxy. The web application includes an inference engine to allow users to upload custom recorded speech for fast inferencing using the production neural network. The app also includes performance and visualization engines that allow users to more closely inspect the data sets used in the development process and to benchmark the model's performance. Functionality has been included to interface with the Microsoft Azure Cognitive Services API's speech to text so that users can benchmark the model against other cloud based speech services. A JavaScript API for automatic speech recognition utilizing the Microsoft Speech-To-Text platform is provided as well. Finally, a sentiment engine was developed using Microsoft's Cognitive Services to allow users to assess how sentiment analysis can be conducted on the model's predicted transcriptions to begin to derive a measure of conversational and therapeutic interactions. It is hypothesized that sentiment analysis could be useful in determining whether or not someone is responding positively to a therapeutic intervention as the content of their responses may be more positive rather than negative.
                                </p>
                            </div>
                            <div class="mdl-cell mdl-cell--12-col">
                                <a href="https://github.com/bricewalker/Hey-Jetson">
                                    <button class="mdl-button mdl-js-button mdl-button--raised mdl-js-ripple-effect mdl-button--accent">
                                        Github
                                    </button>
                                </a>
                            </div>
                        </div>
                    </div>
                </div>
                <footer class="mdl-mini-footer">
                    <div class="mdl-mini-footer__left-section">
                        <div class="mdl-logo">&copy; Copyright 2015-2019 Brice Walker. Design by <a href="https://bricewalker.com" id="tt1">Brice Walker</a><div class="mdl-tooltip" data-mdl-for="tt1">Brice Walker</div>. Created using Python, HTML, CSS, and JavaScript.</div>
                    </div>
                    <div class="mdl-mini-footer__right-sec">
                        <ul class="mdl-mini-footer__link-list">
                            <li><a href="https://github.com/bricewalker"><i class="fab fa-github fa-3x"></i><div class="mdl-tooltip">GitHub</div></a></li>
                            <li><a href="https://www.linkedin.com/in/briceawalker/"><i class="fab fa-linkedin-in fa-3x"></i><div class="mdl-tooltip">Linkedin</div></a></li>
                            <li><a href="https://stackexchange.com/users/11615581/brice-walker"><i class="fab fa-stack-exchange fa-3x"></i><div class="mdl-tooltip">Stack Exchange</div></a></li>
                            <li><a href="https://www.instagram.com/bricewalker/"><i class="fab fa-instagram fa-3x"></i><div class="mdl-tooltip">Instagram</div></a></li>
                        </ul>
                    </div>
                </footer>
            </main>
    </div>
    <script src="https://code.getmdl.io/1.3.0/material.min.js"></script>
</body>
</html>
